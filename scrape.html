<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Ravi Solter, Alexander Traczyk" />


<title>Scraping Data with Selenium</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Natural Language Processing on site comments</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="scrape.html">Scraping Comments</a>
</li>
<li>
  <a href="sent.html">Sentiment Analysis</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Scraping Data with Selenium</h1>
<h4 class="author">Ravi Solter, Alexander Traczyk</h4>

</div>


<p>For us to begin natural language processing and other analysis on the website <a href="https://www.blackwhitereadallover.com/" class="uri">https://www.blackwhitereadallover.com/</a> we of course need to first scrape the necessary data. To do so, we chose to use the selenium package in python. To begin, we need to load required packages, and set up a chrome driver:</p>
<pre class="python"><code># Libraries

import pandas as pd
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import NoSuchElementException
from random import randint
from time import sleep</code></pre>
<pre class="python"><code># Setting up the Chrome webdriver

options = webdriver.ChromeOptions()
options.add_argument(&#39;--headless&#39;) #uncomment to not render driven browser during scrapping

options.binary_location = &quot;/usr/bin/google-chrome&quot;
chrome_driver_binary = &quot;/usr/local/bin/chromedriver&quot;
driver = webdriver.Chrome(chrome_driver_binary, chrome_options=options)</code></pre>
<p>Since each of these scrapers can take quite some time to run, all the data has been saved as csv’s and can be found <a href="https://github.com/rsolter/BWRAO-Comment-Scraper/tree/master/Data">here:</a>.</p>
<div id="scraping-article-names-and-links" class="section level3">
<h3>Scraping Article Names and Links:</h3>
<p>For our scraper to function best in a loop, we needed to get links for all the articles published in the blog. We were lucky enough that the website had a decent <a href="https://www.blackwhitereadallover.com/archives/2007/2">archives page</a> which was then looped through saving all hyperlinks (This list can be found in “links_orig.csv” in our data folder).</p>
<pre class="python"><code># Create a list of archive URLs (Feb, 2007 - March, 2020)

archive_periods = []

for j in range(2007,2021):
    for k in range(1,13):
        single_period = &#39;https://www.blackwhitereadallover.com/archives/&#39;+str(j)+&#39;/&#39;+str(k)
        archive_periods.append(single_period)
        ;
        
# Subsetting out some periods for which there are no articles
active_archives = archive_periods[1:len(archive_periods)-8]

#len(active_archives) #159

#print(active_archives[100])</code></pre>
<pre class="python"><code>## Collecting the article URLS from each archive page, storing in &#39;archive_links&#39; list

archive_links = []

for u in range(0,len(archive_periods)):
    print(u)
    
    sleep(randint(4, 7))  # sleep between 1 and 3 seconds
    
    driver.get(active_archives[u])
    
    
    # Checking to see if &#39;Load More&#39; button exists (for months with&gt;30 articles) and if so clicking it
    button = driver.find_elements_by_xpath(&quot;//button[contains(text(),&#39;Load More&#39;)]&quot;)
    while len(button) &gt; 0 :
        button[0].click()  
        sleep(7)
        button = driver.find_elements_by_xpath(&quot;//button[contains(text(),&#39;Load More&#39;)]&quot;)

 
    
    # Retrieving number of articles from title 
    # OLD archive_title = driver.find_element_by_xpath(&#39;/html/body/div[1]/div[2]/h1&#39;)
    archive_title = driver.find_element_by_xpath(&quot;//h1[contains(text(),&#39;Archives for&#39;)]&quot;)

    #print(archive_title.text)
    
    start = archive_title.text.find(&#39;(&#39;)+1
    start = archive_title.text[start:]
    articles = int(start.replace(&#39;)&#39;,&#39;&#39;))
    #print(articles)
    
    
    # Retrieving the article links themselves
    for q in range(0,articles):
        
        article_link = driver.find_elements_by_css_selector(&#39;h2.c-entry-box--compact__title&gt;a&#39;)
        article_link = article_link[q].get_attribute(&#39;href&#39;)
        
    
        archive_links.append(article_link) 
           
    ;</code></pre>
</div>
<div id="logging-into-bwrao" class="section level3">
<h3>Logging into BWRAO:</h3>
<p>Our next step was to log into the site itself, as this provided more information about the comments (such as user recommendations).</p>
<p>Note: for use, you will need to replace the <code>REPLACE_ME_USER_NAME</code> and <code>REPLACE_ME_PASSWORD</code> in lines 11 and 16</p>
<pre class="python"><code># Logging in to BWRAO to have access to comment recs

driver.get(&quot;https://auth.voxmedia.com/login?return_to=https://www.blackwhitereadallover.com/&quot;)

element = WebDriverWait(driver, 100).until(
        EC.presence_of_element_located((By.CLASS_NAME, &quot;p-text-input&quot;))
    )

username = driver.find_element_by_xpath(&#39;//*[@id=&quot;login[username]&quot;]&#39;)
username.clear()
username.send_keys(&quot;REPLACE_ME_USER_NAME&quot;)


password = driver.find_element_by_xpath(&#39;//*[@id=&quot;login[password]&quot;]&#39;)
password.clear()
password.send_keys(&quot;REPLACE_ME_PASSWORD&quot;)


button = driver.find_element_by_xpath(&#39;//*[@id=&quot;auth&quot;]/div/form/fieldset[3]/input&#39;)
button.click()</code></pre>
<pre class="python"><code># Have selenium wait until the BWRAO logo loads (therefore, has logged in properly)
element = WebDriverWait(driver, 100).until(
        EC.presence_of_element_located((By.CLASS_NAME, &quot;c-global-header__logo-large&quot;))
    )</code></pre>
</div>
<div id="scraping-comments" class="section level3">
<h3>Scraping Comments:</h3>
<p>We now have everything we need to start scraping. In this loop we save save both data about the article itself (title, author, publishing date), and comments (title, text body, poster, date, time, recommendations).</p>
<p>Note: This loop took something like 30 hours between us, to make further analysis easier you can find it saved in our <a href="https://github.com/rsolter/BWRAO-Comment-Scraper/tree/master/Data">data folder</a> as “Comments” 1 through 6.</p>
<pre class="python"><code>##### Gathering blog title, author, publish date and comment list and comment count from a the loop of &#39;archive_links&#39;

comments_df = pd.DataFrame()
for k in range(0,len(archive_links_full)):
#for k in range(1,20):

    
    print(str(k)+&#39; of &#39;+str(len(archive_links_full)))
    
    driver.get(archive_links_full[k])

    #Check for comments, and load, then run everything
    test = len(driver.find_elements_by_xpath(&#39;//*[@id=&quot;comments&quot;]&#39;))
    test2 = len(driver.find_elements_by_xpath(&#39;//*[@class=&quot;p-header-group&quot;]&#39;))
    if (test &gt; 0) &amp; (test2 == 0):
        ## Grab article title, author
        blog_title = driver.find_element_by_xpath(&#39;//*[@id=&quot;content&quot;]/article/div[1]/div[1]/div[2]/h1&#39;)
        article_author = driver.find_element_by_xpath(&#39;//*[@id=&quot;content&quot;]/article/div[1]/div[1]/div[3]/span[1]/span[1]/a/span&#39;)
        article_publish_date = driver.find_element_by_xpath(&#39;//*[@id=&quot;content&quot;]/article/div[1]/div[1]/div[3]/span[1]/span[2]/time&#39;)

        sleep(2)
        driver.execute_script(&quot;window.scrollTo(0, document.body.scrollHeight);&quot;)
        element = WebDriverWait(driver, 100).until(
                EC.presence_of_element_located((By.CLASS_NAME, &quot;c-comments__list&quot;)))

        #Find number of comments

        n_comments = driver.find_element_by_xpath(&#39;//*[@id=&quot;comments&quot;]/div[1]/span/span&#39;)
        n_comments = int(n_comments.text)

        # Looping through all comments to gather comment title, comment body, poster, and number of recs

    

        for i in range(1,n_comments+1):
    
            xpath = &#39;//*[@id=&quot;comments&quot;]/div[2]/div[&#39;+str(i)+&#39;]&#39;
     
            comment_title = driver.find_element_by_xpath(xpath + &#39;/div[1]&#39;)
            comment_body = driver.find_element_by_xpath(xpath + &#39;/div[2]&#39;)
    
            # Generally, comment meta data is stored in the 3rd div except when commentors have a signature
            # in which case the meta data is in the 4th div
            
            #recs aren&#39;t weren&#39;t put in till later, and I was too lazy to find out when
            #so I just check if they exist first
            test = driver.find_elements_by_xpath(xpath+&#39;/div[3]/span[2]/button[2]/span[2]&#39;)
            test2 = driver.find_elements_by_xpath(xpath+&#39;/div[4]/span[2]/button[2]/span[2]&#39;)
            

            if driver.find_elements_by_xpath(xpath + &#39;/div[4]&#39;):
                meta = driver.find_element_by_xpath(xpath+&#39;/div[4]/span[1]&#39;)
                meta = meta.text.rsplit(&#39; on &#39;,1)
                post_date = meta[1]
                poster = meta[0].split(&#39; by &#39;)[1]
    
                if len(test2) &gt; 0:
                # Some comments have no recommendations
                    if driver.find_elements_by_xpath(xpath + &#39;/div[4]/span[2]/button[2]/span[2]&#39;):
                        recs = driver.find_element_by_xpath(xpath + &#39;/div[4]/span[2]/button[2]/span[2]&#39;)
                        recs = int(recs.text.replace(&#39;(&#39;,&#39;&#39;).replace(&#39;)&#39;,&#39;&#39;))
                    else:
                        recs = 0
            else:
                meta = driver.find_element_by_xpath(xpath+&#39;/div[3]/span[1]&#39;)
                meta = meta.text.rsplit(&#39; on &#39;,1)
                post_date = meta[1]
                poster = meta[0].split(&#39; by &#39;)[1]
        
                if len(test) &gt;0:
                # Some comments have no recommendations
                    if driver.find_elements_by_xpath(xpath + &#39;/div[3]/span[2]/button[2]/span[2]&#39;):
                        recs = driver.find_element_by_xpath(xpath + &#39;/div[3]/span[2]/button[2]/span[2]&#39;)
                        recs = int(recs.text.replace(&#39;(&#39;,&#39;&#39;).replace(&#39;)&#39;,&#39;&#39;))
                    else:
                        recs = 0
                        
            if len(test) == 0 &amp; len(test2) == 0:
                recs = &quot;&quot;

            comment = pd.DataFrame([blog_title.text,article_author.text,article_publish_date.text,comment_title.text,comment_body.text,poster,post_date.split(&#39;|&#39;)[0],
                            post_date.split(&#39;|&#39;)[1],recs])
            comments_df = pd.concat([comments_df,comment.T])
    sleep(2)
    ;</code></pre>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
