{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Comments https://www.blackwhitereadallover.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from random import randint\n",
    "from time import sleep \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ravisolter/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: DeprecationWarning: use options instead of chrome_options\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "# Setting up the Chrome webdriver\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless') #uncomment to not render driven browser during scrapping\n",
    "\n",
    "options.binary_location = \"/usr/bin/google-chrome\"\n",
    "chrome_driver_binary = \"/usr/local/bin/chromedriver\"\n",
    "#options.binary_location = \"C:/Program Files (x86)/Google/Chrome/Application/chrome.exe\"\n",
    "#chrome_driver_binary = \"C:/Users/Alek/chromedriver\"\n",
    "driver = webdriver.Chrome(chrome_driver_binary, chrome_options=options)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping the Archives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of archive URLs (Feb, 2007 - March, 2020)\n",
    "\n",
    "archive_periods = []\n",
    "\n",
    "for j in range(2007,2021):\n",
    "    for k in range(1,13):\n",
    "        single_period = 'https://www.blackwhitereadallover.com/archives/'+str(j)+'/'+str(k)\n",
    "        archive_periods.append(single_period)\n",
    "        ;\n",
    "        \n",
    "# Subsetting out some periods for which there are no articles\n",
    "active_archives = archive_periods[1:len(archive_periods)-8]\n",
    "\n",
    "#len(active_archives) #159\n",
    "\n",
    "#print(active_archives[100])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-b852dbc953cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# sleep between 1 and 3 seconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactive_archives\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "## Collecting the article URLS from each archive page, storing in 'archive_links' list\n",
    "\n",
    "archive_links = []\n",
    "\n",
    "for u in range(98,len(archive_periods)):\n",
    "    print(u)\n",
    "    \n",
    "    sleep(randint(4, 7))  # sleep between 1 and 3 seconds\n",
    "    \n",
    "    driver.get(active_archives[u])\n",
    "    \n",
    "    \n",
    "    # Checking to see if 'Load More' button exists (for months with>30 articles) and if so clicking it\n",
    "    button = driver.find_elements_by_xpath(\"//button[contains(text(),'Load More')]\")\n",
    "    while len(button) > 0 :\n",
    "        button[0].click()  \n",
    "        sleep(7)\n",
    "        button = driver.find_elements_by_xpath(\"//button[contains(text(),'Load More')]\")\n",
    "\n",
    " \n",
    "    \n",
    "    # Retrieving number of articles from title \n",
    "    # OLD archive_title = driver.find_element_by_xpath('/html/body/div[1]/div[2]/h1')\n",
    "    archive_title = driver.find_element_by_xpath(\"//h1[contains(text(),'Archives for')]\")\n",
    "\n",
    "    #print(archive_title.text)\n",
    "    \n",
    "    start = archive_title.text.find('(')+1\n",
    "    start = archive_title.text[start:]\n",
    "    articles = int(start.replace(')',''))\n",
    "    #print(articles)\n",
    "    \n",
    "    \n",
    "    # Retrieving the article links themselves\n",
    "    for q in range(0,articles):\n",
    "        \n",
    "        article_link = driver.find_elements_by_css_selector('h2.c-entry-box--compact__title>a')\n",
    "        article_link = article_link[q].get_attribute('href')\n",
    "        \n",
    "    \n",
    "        archive_links.append(article_link) \n",
    "           \n",
    "    ;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6035, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Binding together the resulting .txts. There are 6k articles in total\n",
    "\n",
    "links1 = pd.read_csv('Data/archive_links_1_78.txt',header=None)\n",
    "links2 = pd.read_csv('Data/archive_links_79_97.txt',header=None)\n",
    "links3 = pd.read_csv('Data/archive_links_98_end.txt',header=None)\n",
    "\n",
    "links12 = links1.append(links2, ignore_index = True) \n",
    "archive_links_full = links12.append(links3, ignore_index = True)\n",
    "\n",
    "archive_links_full.head()\n",
    "\n",
    "archive_links_full.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turning it back into a list\n",
    "archive_links_full = archive_links_full[0].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging in to BRWAO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging in to BWRAO to have access to comment recs\n",
    "\n",
    "driver.get(\"https://auth.voxmedia.com/login?return_to=https://www.blackwhitereadallover.com/\")\n",
    "\n",
    "element = WebDriverWait(driver, 100).until(\n",
    "        EC.presence_of_element_located((By.CLASS_NAME, \"p-text-input\"))\n",
    "    )\n",
    "\n",
    "username = driver.find_element_by_xpath('//*[@id=\"login[username]\"]')\n",
    "username.clear()\n",
    "username.send_keys(\"rsolter\")\n",
    "\n",
    "\n",
    "password = driver.find_element_by_xpath('//*[@id=\"login[password]\"]')\n",
    "password.clear()\n",
    "password.send_keys(\"pass123!\")\n",
    "\n",
    "\n",
    "button = driver.find_element_by_xpath('//*[@id=\"auth\"]/div/form/fieldset[3]/input')\n",
    "button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have selenium wait until the BWRAO logo loads (therefore, has logged in properly)\n",
    "element = WebDriverWait(driver, 100).until(\n",
    "        EC.presence_of_element_located((By.CLASS_NAME, \"c-global-header__logo-large\"))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scraping relevant info from individual pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2998 of 6035\n",
      "2999 of 6035\n",
      "3000 of 6035\n"
     ]
    }
   ],
   "source": [
    "##### Gathering blog title, author, publish date and comment list and comment count from a the loop of 'archive_links'\n",
    "\n",
    "comments_df = pd.DataFrame()\n",
    "for k in range(2998,3001):\n",
    "#for k in range(1,20):\n",
    "\n",
    "    \n",
    "    print(str(k)+' of '+str(len(archive_links_full)))\n",
    "    \n",
    "    driver.get(archive_links_full[k])\n",
    "\n",
    "\n",
    "\n",
    "    #Check for comments, and load, then run everything\n",
    "    test = len(driver.find_elements_by_xpath('//*[@id=\"comments\"]'))\n",
    "    test2 = len(driver.find_elements_by_xpath('//*[@class=\"p-header-group\"]'))\n",
    "    if (test > 0) & (test2 == 0):\n",
    "        ## Grab article title, author\n",
    "        blog_title = driver.find_element_by_xpath('//*[@id=\"content\"]/article/div[1]/div[1]/div[2]/h1')\n",
    "        article_author = driver.find_element_by_xpath('//*[@id=\"content\"]/article/div[1]/div[1]/div[3]/span[1]/span[1]/a/span')\n",
    "        article_publish_date = driver.find_element_by_xpath('//*[@id=\"content\"]/article/div[1]/div[1]/div[3]/span[1]/span[2]/time')\n",
    "\n",
    "        sleep(2)\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        element = WebDriverWait(driver, 100).until(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, \"c-comments__list\")))\n",
    "\n",
    "        #Find number of comments\n",
    "\n",
    "        n_comments = driver.find_element_by_xpath('//*[@id=\"comments\"]/div[1]/span/span')\n",
    "        n_comments = int(n_comments.text)\n",
    "\n",
    "\n",
    "        # Looping through all comments to gather comment title, comment body, poster, and number of recs\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "        for i in range(1,n_comments+1):\n",
    "    \n",
    "            xpath = '//*[@id=\"comments\"]/div[2]/div['+str(i)+']'\n",
    "     \n",
    "            comment_title = driver.find_element_by_xpath(xpath + '/div[1]')\n",
    "            comment_body = driver.find_element_by_xpath(xpath + '/div[2]')\n",
    "    \n",
    "            # Generally, comment meta data is stored in the 3rd div except when commentors have a signature\n",
    "            # in which case the meta data is in the 4th div\n",
    "            \n",
    "            #recs aren't weren't put in till later, and I was too lazy to find out when\n",
    "            #so I just check if they exist first\n",
    "            test = driver.find_elements_by_xpath(xpath+'/div[3]/span[2]/button[2]/span[2]')\n",
    "            test2 = driver.find_elements_by_xpath(xpath+'/div[4]/span[2]/button[2]/span[2]')\n",
    "            \n",
    "\n",
    "            if driver.find_elements_by_xpath(xpath + '/div[4]'):\n",
    "                meta = driver.find_element_by_xpath(xpath+'/div[4]/span[1]')\n",
    "                meta = meta.text.rsplit(' on ',1)\n",
    "                post_date = meta[1]\n",
    "                poster = meta[0].split(' by ')[1]\n",
    "    \n",
    "                if len(test2) > 0:\n",
    "                # Some comments have no recommendations\n",
    "                    if driver.find_elements_by_xpath(xpath + '/div[4]/span[2]/button[2]/span[2]'):\n",
    "                        recs = driver.find_element_by_xpath(xpath + '/div[4]/span[2]/button[2]/span[2]')\n",
    "                        recs = int(recs.text.replace('(','').replace(')',''))\n",
    "                    else:\n",
    "                        recs = 0\n",
    "            else:\n",
    "                meta = driver.find_element_by_xpath(xpath+'/div[3]/span[1]')\n",
    "                meta = meta.text.rsplit(' on ',1)\n",
    "                post_date = meta[1]\n",
    "                poster = meta[0].split(' by ')[1]\n",
    "        \n",
    "                if len(test) >0:\n",
    "                # Some comments have no recommendations\n",
    "                    if driver.find_elements_by_xpath(xpath + '/div[3]/span[2]/button[2]/span[2]'):\n",
    "                        recs = driver.find_element_by_xpath(xpath + '/div[3]/span[2]/button[2]/span[2]')\n",
    "                        recs = int(recs.text.replace('(','').replace(')',''))\n",
    "                    else:\n",
    "                        recs = 0\n",
    "                        \n",
    "            if len(test) == 0 & len(test2) == 0:\n",
    "                recs = \"\"\n",
    "\n",
    "\n",
    "            comment = pd.DataFrame([blog_title.text,article_author.text,article_publish_date.text,comment_title.text,comment_body.text,poster,post_date.split('|')[0],\n",
    "                            post_date.split('|')[1],recs])\n",
    "            comments_df = pd.concat([comments_df,comment.T])\n",
    "    sleep(2)\n",
    "    ;\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating into data frame, exporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n"
     ]
    }
   ],
   "source": [
    "comments_df.columns = ['Article Title','Article Author','Article Publish Date','Comment Title','Comment Body','Comment Poster','Comment Date','Comment Time','Comment Recs']\n",
    "comments_df.head()\n",
    "\n",
    "print(k)\n",
    "\n",
    "#comments_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df.to_csv('Data/Comments_2998_3000.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
