---
title: "Scraping Data with Selenium"
author: "Ravi Solter, Alexander Traczyk"
output: html_document
---

For us to begin natural language processing and other analysis on the website https://www.blackwhitereadallover.com/ we of course need to first scrape the necessary data. To do so, we chose to use the selenium package in python. To begin, we need to load required packages, and set up a chrome driver:

```{r setup, include=F}
library(reticulate)
use_python("usr/local/bin/python")
```

```{python, eval=F}
# Libraries

import pandas as pd
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import NoSuchElementException
from random import randint
from time import sleep
```

```{python,eval=F}
# Setting up the Chrome webdriver

options = webdriver.ChromeOptions()
options.add_argument('--headless') #uncomment to not render driven browser during scrapping

options.binary_location = "/usr/bin/google-chrome"
chrome_driver_binary = "/usr/local/bin/chromedriver"
driver = webdriver.Chrome(chrome_driver_binary, chrome_options=options)
```

Since each of these scrapers can take quite some time to run, all the data has been saved as csv's and can be found [here:](https://github.com/rsolter/BWRAO-Comment-Scraper/tree/master/Data).

### Scraping Article Names and Links:

For our scraper to function best in a loop, we needed to get links for all the articles published in the blog. We were lucky enough that the website had a decent [archives page](https://www.blackwhitereadallover.com/archives/2007/2) which was then looped through saving all hyperlinks (This list can be found in "links_orig.csv" in our data folder).

```{python, eval=F}
# Create a list of archive URLs (Feb, 2007 - March, 2020)

archive_periods = []

for j in range(2007,2021):
    for k in range(1,13):
        single_period = 'https://www.blackwhitereadallover.com/archives/'+str(j)+'/'+str(k)
        archive_periods.append(single_period)
        ;
        
# Subsetting out some periods for which there are no articles
active_archives = archive_periods[1:len(archive_periods)-8]

#len(active_archives) #159

#print(active_archives[100])
```
```{python, eval=F}
## Collecting the article URLS from each archive page, storing in 'archive_links' list

archive_links = []

for u in range(0,len(archive_periods)):
    print(u)
    
    sleep(randint(4, 7))  # sleep between 1 and 3 seconds
    
    driver.get(active_archives[u])
    
    
    # Checking to see if 'Load More' button exists (for months with>30 articles) and if so clicking it
    button = driver.find_elements_by_xpath("//button[contains(text(),'Load More')]")
    while len(button) > 0 :
        button[0].click()  
        sleep(7)
        button = driver.find_elements_by_xpath("//button[contains(text(),'Load More')]")

 
    
    # Retrieving number of articles from title 
    # OLD archive_title = driver.find_element_by_xpath('/html/body/div[1]/div[2]/h1')
    archive_title = driver.find_element_by_xpath("//h1[contains(text(),'Archives for')]")

    #print(archive_title.text)
    
    start = archive_title.text.find('(')+1
    start = archive_title.text[start:]
    articles = int(start.replace(')',''))
    #print(articles)
    
    
    # Retrieving the article links themselves
    for q in range(0,articles):
        
        article_link = driver.find_elements_by_css_selector('h2.c-entry-box--compact__title>a')
        article_link = article_link[q].get_attribute('href')
        
    
        archive_links.append(article_link) 
           
    ;
```

### Logging into BWRAO:

Our next step was to log into the site itself, as this provided more information about the comments (such as user recommendations).

Note: for use, you will need to replace the `REPLACE_ME_USER_NAME` and `REPLACE_ME_PASSWORD` in lines 11 and 16
```{python, eval=F}
# Logging in to BWRAO to have access to comment recs

driver.get("https://auth.voxmedia.com/login?return_to=https://www.blackwhitereadallover.com/")

element = WebDriverWait(driver, 100).until(
        EC.presence_of_element_located((By.CLASS_NAME, "p-text-input"))
    )

username = driver.find_element_by_xpath('//*[@id="login[username]"]')
username.clear()
username.send_keys("REPLACE_ME_USER_NAME")


password = driver.find_element_by_xpath('//*[@id="login[password]"]')
password.clear()
password.send_keys("REPLACE_ME_PASSWORD")


button = driver.find_element_by_xpath('//*[@id="auth"]/div/form/fieldset[3]/input')
button.click()
```
```{python, eval=F}
# Have selenium wait until the BWRAO logo loads (therefore, has logged in properly)
element = WebDriverWait(driver, 100).until(
        EC.presence_of_element_located((By.CLASS_NAME, "c-global-header__logo-large"))
    )
```

### Scraping Comments:

We now have everything we need to start scraping. In this loop we save save both data about the article itself (title, author, publishing date), and comments (title, text body, poster, date, time, recommendations).

Note: This loop took something like 30 hours between us, to make further analysis easier you can find it saved in our [data folder](https://github.com/rsolter/BWRAO-Comment-Scraper/tree/master/Data) as "Comments" 1 through 6.
```{python, eval=F}

##### Gathering blog title, author, publish date and comment list and comment count from a the loop of 'archive_links'

comments_df = pd.DataFrame()
for k in range(0,len(archive_links_full)):
#for k in range(1,20):

    
    print(str(k)+' of '+str(len(archive_links_full)))
    
    driver.get(archive_links_full[k])



    #Check for comments, and load, then run everything
    test = len(driver.find_elements_by_xpath('//*[@id="comments"]'))
    test2 = len(driver.find_elements_by_xpath('//*[@class="p-header-group"]'))
    if (test > 0) & (test2 == 0):
        ## Grab article title, author
        blog_title = driver.find_element_by_xpath('//*[@id="content"]/article/div[1]/div[1]/div[2]/h1')
        article_author = driver.find_element_by_xpath('//*[@id="content"]/article/div[1]/div[1]/div[3]/span[1]/span[1]/a/span')
        article_publish_date = driver.find_element_by_xpath('//*[@id="content"]/article/div[1]/div[1]/div[3]/span[1]/span[2]/time')

        sleep(2)
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        element = WebDriverWait(driver, 100).until(
                EC.presence_of_element_located((By.CLASS_NAME, "c-comments__list")))

        #Find number of comments

        n_comments = driver.find_element_by_xpath('//*[@id="comments"]/div[1]/span/span')
        n_comments = int(n_comments.text)


        # Looping through all comments to gather comment title, comment body, poster, and number of recs


    

        for i in range(1,n_comments+1):
    
            xpath = '//*[@id="comments"]/div[2]/div['+str(i)+']'
     
            comment_title = driver.find_element_by_xpath(xpath + '/div[1]')
            comment_body = driver.find_element_by_xpath(xpath + '/div[2]')
    
            # Generally, comment meta data is stored in the 3rd div except when commentors have a signature
            # in which case the meta data is in the 4th div
            
            #recs aren't weren't put in till later, and I was too lazy to find out when
            #so I just check if they exist first
            test = driver.find_elements_by_xpath(xpath+'/div[3]/span[2]/button[2]/span[2]')
            test2 = driver.find_elements_by_xpath(xpath+'/div[4]/span[2]/button[2]/span[2]')
            

            if driver.find_elements_by_xpath(xpath + '/div[4]'):
                meta = driver.find_element_by_xpath(xpath+'/div[4]/span[1]')
                meta = meta.text.rsplit(' on ',1)
                post_date = meta[1]
                poster = meta[0].split(' by ')[1]
    
                if len(test2) > 0:
                # Some comments have no recommendations
                    if driver.find_elements_by_xpath(xpath + '/div[4]/span[2]/button[2]/span[2]'):
                        recs = driver.find_element_by_xpath(xpath + '/div[4]/span[2]/button[2]/span[2]')
                        recs = int(recs.text.replace('(','').replace(')',''))
                    else:
                        recs = 0
            else:
                meta = driver.find_element_by_xpath(xpath+'/div[3]/span[1]')
                meta = meta.text.rsplit(' on ',1)
                post_date = meta[1]
                poster = meta[0].split(' by ')[1]
        
                if len(test) >0:
                # Some comments have no recommendations
                    if driver.find_elements_by_xpath(xpath + '/div[3]/span[2]/button[2]/span[2]'):
                        recs = driver.find_element_by_xpath(xpath + '/div[3]/span[2]/button[2]/span[2]')
                        recs = int(recs.text.replace('(','').replace(')',''))
                    else:
                        recs = 0
                        
            if len(test) == 0 & len(test2) == 0:
                recs = ""


            comment = pd.DataFrame([blog_title.text,article_author.text,article_publish_date.text,comment_title.text,comment_body.text,poster,post_date.split('|')[0],
                            post_date.split('|')[1],recs])
            comments_df = pd.concat([comments_df,comment.T])
    sleep(2)
    ;
```